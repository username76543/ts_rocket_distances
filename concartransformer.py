"""ConCar transformer."""

__maintainer__ = ["NHarner"]
__all__ = ["ConCar"]

import numpy as np
from numba import get_num_threads, njit, prange, set_num_threads

from aeon.transformations.collection import BaseCollectionTransformer, Normalizer
from aeon.utils.validation import check_n_jobs

#msm distance is numba compatible, msm_pairwise_distance is not.
#This increases runtime for initial POI selection
from aeon.distances import msm_distance

class ConCarTransformer(BaseCollectionTransformer):
    """Convolutional Cartography: Combining ROCKET and Distance Based Learning.
    
    RandOm Convolutional KErnel Transform (ROCKET).

    A kernel (or convolution) is a subseries used to create features that can be used
    in machine learning tasks. ROCKET [1]_ generates a large number of random
    convolutional kernels in the fit method. The length and dilation of each kernel
    are also randomly generated. A kernel is used to create an activation map for
    each series by running it across a time series, including random length and
    dilation. It transforms the time series and is used as a randomization method
    to produce a transformed set of datapoints.
    
    Cartography
    
    After convolving a series, new features are generated by comparing the series
    to a number of reference Points of Interest. These points are chosen randomly,
    selected as good initial cluster centers, or via clustering (under development).
    Reference points are compared with a distance measure and the resulting distances
    are the pooling features for ROCKET.  Currently uses the Move-Split-Merge
    (MSM) metric[2]. Only independent distance calculations for multivariate series are
    available, since ROCKET may zero some channels.
    
    #Preliminary Results
    
    From small scale testing, random convolved point distances are good pooling features  
    for ROCKET, but require substantially more computation. Accuracy is better than NN 
    or EE, but worse than MiniRocket, with less runtime that EE, but more than ROCKET. 
    
    #Currently in Alpha, plan to improve and remain compatible with Aeon toolkit.
    
    TODO
    
    Tuneable POI Inital Search Fraction
    POI chosen via clustering (will increase runtime)
    MiniRocket Kernels
    Other Distance Measures (TWE, DTW and friends, priority from [3])
    Improve Convolution Function (will decrease runtime)
    Make pairwise distance numba compatible (will require aeon support)
    Code Cleanup

    Parameters
    ----------
    n_kernels : int, default=512
       Number of random convolutional kernels.
    normalise : bool, default True
       Whether or not to normalise the input time series per instance.
    n_jobs : int, default=1
       The number of jobs to run in parallel for `transform`. ``-1`` means using all
       processors.
    random_state : None or int, optional, default = None
        Seed for random number generation.
    poi_per_kernel : string, default=log_log_2
        Default chooses ceil(log2(log2(|X|))), where X is the test dataset.
        options include: log_2, log_log_2
    window_frac: float, default = .2
        Bounding matrix window size for elastic distance measures
        
        
    See Also
    --------
    MiniRocket, MultiRocket, HYDRA

    References
    ----------
    .. [1] Tan, Chang Wei and Dempster, Angus and Bergmeir, Christoph
        and Webb, Geoffrey I,
        "ROCKET: Exceptionally fast and accurate time series
      classification using random convolutional kernels",2020,
      https://link.springer.com/article/10.1007/s10618-020-00701-z,
      https://arxiv.org/abs/1910.1305
    
    ..[2] Stefan A., Athitsos V., Das G.: 
        "The Move-Split-Merge metric for time series.", 
        IEEE Transactions on Knowledge and Data Engineering 25(6), 2013.
        https://ranger.uta.edu/~alex/publications/stefan_tkde2012_preprint.pdf
     
    ..[3] Holder, Christopher and Middlehurst, Matthew and Bagnall, Anthony,
        "A review and evaluation of elastic distance functions for time series
         clustering"
        https://arxiv.org/abs/2205.15181

    Example
    --------
    >>> from from concartransformer import ConCarTransformer
    >>> from aeon.datasets import load_unit_test
    >>> X_train, y_train = load_unit_test(split="train")
    >>> X_test, y_test = load_unit_test(split="test")
    >>> trf = ConCarTransformer(n_kernels=512)
    >>> trf.fit(X_train)
    >>> X_train = trf.transform(X_train)
    >>> X_test = trf.transform(X_test)
    """

    _tags = {
        "output_data_type": "Tabular",
        "capability:multivariate": True,
        "capability:multithreading": True,
        "algorithm_type": "convolution",
        "X_inner_type": "numpy3D",
    }

    def __init__(
        self,
        n_kernels=512,
        normalise=True,
        n_jobs=1,
        random_state=None,
        poi_per_kernel = 'log_log_2',
        window_frac = .2
    ):
        self.n_kernels = n_kernels
        self.normalise = normalise
        self.n_jobs = n_jobs
        self.random_state = random_state
        self.poi_per_kernel = poi_per_kernel
        self.window_frac = window_frac
        super().__init__()

    def _fit(self, X, y=None):
        """Generate random kernels adjusted to time series shape.

        Infers time series length and number of channels from input numpy array,
        and generates random kernels.

        Parameters
        ----------
        X : 3D np.ndarray of shape = (n_cases, n_channels, n_timepoints)
            collection of time series to transform
        y : ignored argument for interface compatibility

        Returns
        -------
        self
        """
        self._n_jobs = check_n_jobs(self.n_jobs)

        if isinstance(self.random_state, int):
            self._random_state = self.random_state
        else:
            self._random_state = None
        n_channels = X[0].shape[0]

        # The only use of n_timepoints is to set the maximum dilation
        self.fit_min_length_ = X[0].shape[1]
        self.kernels = _generate_kernels(
            self.fit_min_length_, self.n_kernels, n_channels, self._random_state
        )
        self.dist_name = "msm"
        self.kernel_points = _get_points_of_interest(X, self.kernels, self.poi_per_kernel, self.dist_name, self.window_frac)
        return self

    def _transform(self, X, y=None):
        """Transform input time series using random convolutional kernels.

        Parameters
        ----------
        X : 3D np.ndarray of shape = (n_cases, n_channels, n_timepoints)
            collection of time series to transform
        y : ignored argument for interface compatibility

        Returns
        -------
        np.ndarray (n_cases, n_kernels), transformed features
        """
        if self.normalise:
            norm = Normalizer()
            X = norm.fit_transform(X)

        prev_threads = get_num_threads()
        set_num_threads(self._n_jobs)
        #print("before apply kernels")
        X_ = _apply_kernels_poi(X, self.kernels, self.kernel_points, self.dist_name, self.window_frac)
        #print("after apply kernels")
        set_num_threads(prev_threads)
        return X_


@njit(fastmath=True, cache=True)
def _generate_kernels(n_timepoints, n_kernels, n_channels, seed):
    if seed is not None:
        np.random.seed(seed)
    candidate_lengths = np.array((7, 9, 11), dtype=np.int32)
    lengths = np.random.choice(candidate_lengths, n_kernels).astype(np.int32)

    num_channel_indices = np.zeros(n_kernels, dtype=np.int32)
    for i in range(n_kernels):
        limit = min(n_channels, lengths[i])
        num_channel_indices[i] = 2 ** np.random.uniform(0, np.log2(limit + 1))

    channel_indices = np.zeros(num_channel_indices.sum(), dtype=np.int32)

    weights = np.zeros(
        np.int32(
            np.dot(lengths.astype(np.float32), num_channel_indices.astype(np.float32))
        ),
        dtype=np.float32,
    )
    biases = np.zeros(n_kernels, dtype=np.float32)
    dilations = np.zeros(n_kernels, dtype=np.int32)
    paddings = np.zeros(n_kernels, dtype=np.int32)

    a1 = 0  # for weights
    a2 = 0  # for channel_indices

    for i in range(n_kernels):
        _length = lengths[i]
        _num_channel_indices = num_channel_indices[i]

        _weights = np.random.normal(0, 1, _num_channel_indices * _length).astype(
            np.float32
        )

        b1 = a1 + (_num_channel_indices * _length)
        b2 = a2 + _num_channel_indices

        a3 = 0  # for weights (per channel)
        for _ in range(_num_channel_indices):
            b3 = a3 + _length
            _weights[a3:b3] = _weights[a3:b3] - _weights[a3:b3].mean()
            a3 = b3

        weights[a1:b1] = _weights

        channel_indices[a2:b2] = np.random.choice(
            np.arange(0, n_channels), _num_channel_indices, replace=False
        )

        biases[i] = np.random.uniform(-1, 1)

        dilation = 2 ** np.random.uniform(
            0, np.log2((n_timepoints - 1) / (_length - 1))
        )
        dilation = np.int32(dilation)
        dilations[i] = dilation

        padding = ((_length - 1) * dilation) // 2 if np.random.randint(2) == 1 else 0
        paddings[i] = padding

        a1 = b1
        a2 = b2

    return (
        weights,
        lengths,
        biases,
        dilations,
        paddings,
        num_channel_indices,
        channel_indices,
    )

@njit(
    parallel=True,
    fastmath=True,
    cache=True,
)
def _apply_kernels_poi(X, kernels, pois, distance_func, window_frac):
    (
        weights,
        lengths,
        biases,
        dilations,
        paddings,
        n_channel_indices,
        channel_indices,
    ) = kernels
    n_cases = len(X)
    n_channels, _ = X[0].shape
    n_kernels = len(lengths)
    poi_per_kernel = pois.shape[1]
    return_features = np.zeros((n_cases, n_kernels * poi_per_kernel), dtype=np.float32)

    for i in prange(n_cases):
        a1 = 0  # for weights
        a2 = 0  # for channel_indices
        a3 = 0  # for features
        
        for j in range(n_kernels):
            b1 = a1 + n_channel_indices[j] * lengths[j]
            b2 = a2 + n_channel_indices[j]
            b3 = a3 + poi_per_kernel

            if n_channel_indices[j] == 1:
                new_features = _apply_poi_kernel_univariate(
                    X[i][channel_indices[a2]],
                    weights[a1:b1],
                    lengths[j],
                    biases[j],
                    dilations[j],
                    paddings[j],
                    pois[j],
                    distance_func,
                    window_frac
                )
                for k in range(0, poi_per_kernel):
                    return_features[i][j*poi_per_kernel+k] = new_features[k]
                

            else:
                #Implement Multivariate here
                Nothing=None
                reshaped_weights = weights[a1:b1].reshape(n_channel_indices[j], lengths[j])
                new_features = _apply_poi_kernel_multivariate(
                X[i],
                reshaped_weights,
                lengths[j],
                biases[j],
                dilations[j],
                paddings[j],
                n_channel_indices[j],
                channel_indices[a2:b2],
                pois[j],
                distance_func,
                window_frac
                )
                for k in range(0, poi_per_kernel):
                    return_features[i][j*poi_per_kernel+k] = new_features[k]

            a1 = b1
            a2 = b2
            a3 = b3
    #print("after prange")
    #print(return_features)
    return return_features.astype(np.float32)

@njit(
    parallel=True,
    fastmath=True,
    cache=True,
)
def _get_points_of_interest(X, kernels, poi_per_kernel="log_log_2", pw_dist_name='msm', window_frac=.2):
    (
        weights,
        lengths,
        biases,
        dilations,
        paddings,
        n_channel_indices,
        channel_indices,
    ) = kernels
    n_cases = len(X)
    if poi_per_kernel == "log_log_2":
        n_cases = int(np.ceil(np.log2(np.log2(n_cases))))
    elif poi_per_kernel == "log_2":
        n_cases = int(np.ceil(np.log2(n_cases)))
    else:
        n_cases = 1
    #n_cases = int(np.ceil(np.log2(n_cases)))
    search_buffer = n_cases
    #print("n_cases", n_cases)
    n_channels, n_timepoints = X[0].shape
    n_kernels = len(lengths)
    #print('n_kernels', n_kernels)
    weight_starts = np.zeros(n_kernels)
    weight_ends = np.zeros(n_kernels)
    channel_starts = np.zeros(n_kernels)
    channel_ends = np.zeros(n_kernels)
    a1_i = 0
    a2_i = 0
    for i in range(n_kernels):
        weight_starts[i] = a1_i
        a1_i = a1_i + n_channel_indices[i] * lengths[i]
        weight_ends[i] = a1_i
        channel_starts[i] = a2_i
        a2_i = a2_i + n_channel_indices[i]
        channel_ends[i] = a2_i
        
    return_points = np.zeros((n_kernels, n_cases, n_channels, n_timepoints), dtype=np.float32)
    for i in prange(n_kernels):
        a1 = 0  # for weights
        a2 = 0  # for channel_indices
        a3 = 0  # for features

        candidate_points = np.zeros((n_cases, n_channels, n_timepoints), dtype=np.float32)
        #eval_candidate_buffer = np.zeros((n_channels, n_timepoints), dtype=np.float32)
        current_distances = np.zeros((n_cases, n_cases), dtype=np.float32)
        candidate_indices = np.random.choice(np.arange(0, len(X)), size=n_cases+search_buffer, replace=False)
        initial_candidates = 0
        for j in range(len(candidate_indices)):
            #print("candidate example", X[candidate_indices[j]])
            print("candidate_points", candidate_points.shape)
            if n_channel_indices[i] == 1:
                eval_candidate_buffer = np.zeros((n_channels, n_timepoints), dtype=np.float32)
                convolved_candidate = np.zeros(len(X[candidate_indices[j]][0]), dtype=np.float32)
                convolved_candidate = univariate_apply_kernel_return_convolved(
                X[candidate_indices[j]][0],
                weights[weight_starts[i]:weight_ends[i]],
                lengths[i],
                biases[i],
                dilations[i],
                paddings[i],
                convolved_candidate
                )
                if initial_candidates < n_cases:
                    candidate_points[initial_candidates] = convolved_candidate
                    initial_candidates += 1
                    if initial_candidates == n_cases:
                        if pw_dist_name == 'msm':
                            #msm_distance is numba compatible, msm_pairwise is not.
                            #because of this, I have to individually compute the initial pairwise matrix
                            for row_index in range(0, len(candidate_points)):
                                for column_index in range(row_index+1, len(candidate_points)):
                                    if row_index == column_index:
                                        continue
                                    else:
                                        new_dist = msm_distance(candidate_points[row_index], candidate_points[column_index], window=window_frac)
                                        current_distances[row_index][column_index] = new_dist
                                        current_distances[column_index][row_index] = new_dist
                                #print(row_index, column_index, current_distances)
                        else:
                            nothing = None #raise ValueError("Invalid dist func provided.")
                elif initial_candidates == n_cases:
                    eval_candidate_buffer[0] = convolved_candidate
                    candidate_points, current_distances = accept_or_reject_candidate(candidate_points, eval_candidate_buffer, current_distances, pw_dist_name, window_frac)
            else:
                #print("multivariate_series", X[candidate_indices[j]])
                #print(X[candidate_indices[j]].shape)
                reshaped_weights = weights[weight_starts[i]:weight_ends[i]].reshape(n_channel_indices[i], lengths[i])
                convolved_candidate = np.zeros((X[candidate_indices[j]].shape[0], X[candidate_indices[j]].shape[1]), dtype=np.float32)
                convolved_candidate = multivariate_apply_kernel_return_convolved(
                X[candidate_indices[j]],
                reshaped_weights,
                lengths[i],
                biases[i],
                dilations[i],
                paddings[i],
                n_channel_indices[i],
                channel_indices[channel_starts[i]:channel_ends[i]],
                convolved_candidate
                )
                #print(convolved_candidate)
                if initial_candidates < n_cases:
                    candidate_points[initial_candidates] = convolved_candidate
                    initial_candidates += 1
                    if initial_candidates == n_cases:
                        if pw_dist_name == 'msm':
                            #msm_distance is numba compatible, msm_pairwise is not.
                            #because of this, I have to individually compute the initial pairwise matrix
                            for row_index in range(0, len(candidate_points)):
                                for column_index in range(row_index+1, len(candidate_points)):
                                    if row_index == column_index:
                                        continue
                                    else:
                                        new_dist = msm_distance(candidate_points[row_index], candidate_points[column_index], window=window_frac)
                                        current_distances[row_index][column_index] = new_dist
                                        current_distances[column_index][row_index] = new_dist
                                #print(row_index, column_index, current_distances)
                        else:
                            nothing = None #raise ValueError("Invalid dist func provided.")
                elif initial_candidates == n_cases:
                    #eval_candidate_buffer[0] = convolved_candidate
                    candidate_points, current_distances = accept_or_reject_candidate(candidate_points, convolved_candidate, current_distances, pw_dist_name, window_frac)
        return_points[i] = candidate_points
    
    #print(return_points)
    return return_points
    
    
#Function to add additional initial poi to pool without clustering
#Tries to maximize how "triangular" they are by picking points that are uniformly
#distant from current points    
@njit(fastmath=True, cache=True)
def accept_or_reject_candidate(candidate_points, convolved_candidate, current_distances, my_pw_dist_func, window_frac=.2):
    current_distances_scores = score_pois_from_dist_matrix(current_distances)
    #print('scores', current_distances_scores)
    worst_poi = np.argmin(current_distances_scores)
    n_candidates = candidate_points.shape[0]
    candidate_distances = np.zeros((n_candidates), dtype=np.float32)
    for poi_index in range(n_candidates):
        if poi_index == worst_poi:
            continue
        else:
            if my_pw_dist_func == 'msm':
                #print(convolved_candidate)
                #print(candidate_points[poi_index])
                new_dist = msm_distance(candidate_points[poi_index], convolved_candidate, window=window_frac)
                #print("new_dist", new_dist)
                candidate_distances[poi_index] = new_dist
            else:
                nothing = None
    candidate_total_distances = sum(candidate_distances)
    mean_dist = candidate_total_distances/(n_candidates-1)
    nonuniformity = 0
    for poi_index in range(candidate_points.shape[0]):
        if poi_index == worst_poi:
            continue
        else:
            nonuniformity += abs(candidate_distances[poi_index]-mean_dist)
    divergence_from_uniformity_penalty = (1-nonuniformity/candidate_total_distances)
    new_candidate_score = candidate_total_distances*divergence_from_uniformity_penalty
    if new_candidate_score > current_distances_scores[worst_poi]:
        for row_index in range(0, n_candidates):
            if row_index == worst_poi:
                for column_index in range(0, n_candidates):
                    current_distances[row_index][column_index] = candidate_distances[column_index]
            else:
                current_distances[row_index][worst_poi] = candidate_distances[row_index]
                
        candidate_points[worst_poi] = convolved_candidate
    return candidate_points, current_distances
    
@njit(fastmath=True, cache=True)
def score_pois_from_dist_matrix(dist_matrix):
    poi_count = dist_matrix.shape[0]
    scores = np.zeros((poi_count), dtype=np.float32)
    for row in range(poi_count):
        total_dist = sum(dist_matrix[row])
        #the diagonals are all 0, so you skip them to find the mean of different poi
        mean_dist = total_dist/(poi_count-1)
        nonuniformity = 0
        for column_index in range(poi_count):
            if row == column_index:
                continue
            nonuniformity += abs(dist_matrix[row][column_index]-mean_dist)
        divergence_from_uniformity_penalty = (1-nonuniformity/total_dist)
        #print(row, dist_matrix[row], poi_count, total_dist, mean_dist, nonuniformity, divergence_from_uniformity_penalty)
        #I want points which are far from my current points, but uniformly so)
        scores[row] = total_dist*divergence_from_uniformity_penalty
    return scores
    
@njit(fastmath=True, cache=True)
def univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, return_series):
    """Apply a single kernel to a univariate series."""
    n_timepoints = len(X)
    offsets = np.zeros(length)
    midpoint = int(length/2)
    for i in range(length):
        offsets[i] = np.int32(i-midpoint+(i-midpoint)*dilation)
    for i in range(n_timepoints):
        for j in range(length):
            if i+offsets[j] > -1 and i + offsets[j] < n_timepoints:
                return_series[i] += np.float32(X[int(i+offsets[j])]*weights[j])
    return return_series
    
@njit(fastmath=True, cache=True)
def multivariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, num_channel_indices, channel_indices, return_series):
    """Apply a single kernel to a multivariate series."""
    #print(X.shape, weights.shape, channel_indices.shape)
    #print(channel_indices)
    #print(weights.shape)#, length, bias, dilation, padding, return_series)
    n_channels, n_timepoints = X.shape
    offsets = np.zeros(n_timepoints)
    midpoint = int(length/2)
    for i in range(length):
        offsets[i] = np.int32(i-midpoint+(i-midpoint)*dilation)
    for i in range(n_timepoints):
        for j in range(length):
            for k in range(num_channel_indices):
                if i+offsets[j] > -1 and i + offsets[j] < n_timepoints:
                    return_series[channel_indices[k]][i] += np.float32(X[channel_indices[k]][int(i+offsets[j])]*weights[k][j])
    return return_series

@njit(fastmath=True, cache=True)
def _apply_poi_kernel_univariate(X, weights, length, bias, dilation, padding, pois, my_pw_dist_func, window_frac=.2):
    """Measure a single set of distances for univariate time series pois under convolution."""
    dist_buffer = np.zeros((1, len(X)), dtype=np.float32)
    convolved_X = np.zeros(len(X), dtype=np.float32)
    convolved_x = univariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, convolved_X)
    dist_buffer[0] = convolved_x
    return_array = np.zeros(pois.shape[0], dtype=np.float32)
    #print("return_array", return_array)
    for i in range(0, pois.shape[0]):
        if my_pw_dist_func == 'msm':
            #print("measure shapes", dist_buffer.shape, pois[i].shape)
            new_dist = msm_distance(dist_buffer, pois[i], window=window_frac)
            #print("new_dist", new_dist)
            return_array[i] = new_dist
        else:
            raise ValueError("Invalid dist func provided.")

    return return_array

@njit(fastmath=True, cache=True)
def _apply_poi_kernel_multivariate(X, weights, length, bias, dilation, padding, num_channel_indices, channel_indices, pois, my_pw_dist_func, window_frac=.2):
    """Measure a single set of distances for multivariate time series pois under convolution."""
    convolved_X = np.zeros((X.shape[0], X.shape[1]), dtype=np.float32)
    convolved_x = multivariate_apply_kernel_return_convolved(X, weights, length, bias, dilation, padding, num_channel_indices, channel_indices, convolved_X)
    return_array = np.zeros(pois.shape[0], dtype=np.float32)
    #print("return_array", return_array)
    for i in range(0, pois.shape[0]):
        if my_pw_dist_func == 'msm':
            #print("measure shapes", dist_buffer.shape, pois[i].shape)
            new_dist = msm_distance(convolved_x, pois[i], window=window_frac)
            #print("new_dist", new_dist)
            return_array[i] = new_dist
        else:
            raise ValueError("Invalid dist func provided.")

    return return_array

@njit(fastmath=True, cache=True)
def _apply_kernel_multivariate(
    X, weights, length, bias, dilation, padding, num_channel_indices, channel_indices
):
    """Apply a kernel to a single multivariate time series."""
    #Need to replace for multivariate capabilities
    n_columns, n_timepoints = X.shape

    output_length = (n_timepoints + (2 * padding)) - ((length - 1) * dilation)

    _ppv = 0
    _max = -np.inf
    end = (n_timepoints + padding) - ((length - 1) * dilation)
    for i in range(-padding, end):
        _sum = bias
        index = i
        for j in range(length):
            if index > -1 and index < n_timepoints:
                for k in range(num_channel_indices):
                    _sum = _sum + weights[k, j] * X[channel_indices[k], index]
            index = index + dilation
        if _sum > _max:
            _max = _sum
        if _sum > 0:
            _ppv += 1
    return np.float32(_ppv / output_length), np.float32(_max)
